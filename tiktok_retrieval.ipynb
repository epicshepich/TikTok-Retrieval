{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30c0ae1",
   "metadata": {},
   "source": [
    "# Best Practices in TikTok Retrieval\n",
    "### Jim Shepich III\n",
    "### 3 December 2022\n",
    "### Time Required: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbcd3a0",
   "metadata": {},
   "source": [
    "This notebook contains experiments that explore best practices in TikTok retrieval, as well as the analysis of their results.\n",
    "\n",
    "Note: by convention, I will use decimal prefixes for file sizes (i.e. 1KB = 1000B; 1MB = 1000KB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ee6cc",
   "metadata": {},
   "source": [
    "# Contents <a id=\"contents\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c231906",
   "metadata": {},
   "source": [
    "- [Import Packages](#import-packages)\n",
    "- [Corpus Processing](#corpus-processing)\n",
    "- [Normalization](#normalization)\n",
    "- [Combinations of Features](#features)\n",
    "- [Index Construction](#index-construction)\n",
    "- [Query Vectorization](#query-vectorization)\n",
    "- [Cosine Scoring](#cosine-scoring)\n",
    "- [Experiments](#experiments)\n",
    "- [Analyses](#analyses)\n",
    "    - [Index Construction & Query Time](#anal-time)\n",
    "    - [Index Storage](#anal-storage)\n",
    "    - [Performance vs Query Properties](#anal-queries)\n",
    "    - [Performance vs Indexing Methodolgy](#anal-indexing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ebf83",
   "metadata": {},
   "source": [
    "# Import Packages <a name=\"import-packages\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208db4a6",
   "metadata": {},
   "source": [
    "Note: all code in this assignment is implemented in Julia version 1.8.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96089957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using DataFrames #For pretty-printing results\n",
    "ENV[\"COLUMNS\"] = 10000 #Show all DataFrame columns.\n",
    "using Plots; pyplot() #For generating plots\n",
    "using WebIO\n",
    "using StatsBase #For basic statistical functions like mean and std\n",
    "using Combinatorics #For power set implementation\n",
    "using BenchmarkTools #For measuring performance\n",
    "using JSON #For saving/loading results.\n",
    "using Unidecode #For converting emojis and Greek characters into English words.\n",
    "using PyCall #For accessing Python's \"unidecode\" library\n",
    "pyunidecode = pyimport(\"unidecode\")\n",
    "using Languages #For stopwords\n",
    "using SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fcf49",
   "metadata": {},
   "source": [
    "Additionally, in order to facilitate the reporting of time/memory results, I'm going to introduce a function that I've developed previously to apply a scientific prefix to a number (e.g. convert bytes to KB, MB, GB as appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee97e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scifmt (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sciprefix(measurement::Number;maxpower=Inf)\n",
    "    \"\"\"Takes a measurement and determines the appropriate scientific\n",
    "    prefix. The `maxpower` keyword sets the largest power that will be considered.\n",
    "    \"\"\"\n",
    "    prefixes = [\n",
    "        (18,\"E\"),\n",
    "        (15,\"P\"),\n",
    "        (12,\"T\"),\n",
    "        (9,\"G\"),\n",
    "        (6,\"M\"),\n",
    "        (3,\"k\"),\n",
    "        (0,\"\"),\n",
    "        (-3,\"m\"),\n",
    "        (-6,\"Î¼\"),\n",
    "        (-9,\"n\"),\n",
    "        (-12,\"p\"),\n",
    "        (-15,\"f\")\n",
    "    ]\n",
    "    for (power,prefix) in prefixes\n",
    "        if (power>maxpower)\n",
    "            continue\n",
    "        end\n",
    "        if measurement >= 10.0^power\n",
    "            return (measurement/(10.0^power),prefix)\n",
    "            #Find the first prefixed power of 10 that the\n",
    "            #measurement is greater than, then return the measurement\n",
    "            #in those units and that prefix.\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function scifmt(measurement::Number; digits=2, unit=\"u\", kwargs...)\n",
    "    \"\"\"Uses `scientific_prefix` to report a measurement with\n",
    "    the appropriate scientific prefix before a specified unit abbreviation,\n",
    "    rounded to a specified decimal place.\"\"\"\n",
    "    value, prefix = sciprefix(measurement;kwargs...)\n",
    "    return \"$(round(value,digits=digits)) $(prefix)$(unit)\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d0728",
   "metadata": {},
   "source": [
    "# Corpus Processing <a id=\"corpus-processing\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4b047",
   "metadata": {},
   "source": [
    "The data we will use in these experiments are derived from a small collection of TikToks that I scraped from my bank of liked videos. Details on the scraping process are provided in `README.md`. Specifically, we will be studying text data extracted from the TikToks; after cleaning and processing, this data is stored in `data/clean_master.json`. Each TikTok contains the following text fields for potential indexing:\n",
    "\n",
    "- Basic information: creator username and nickname, description text (including hashtags), audio title\n",
    "- Comment text and commenter usernames\n",
    "- Text extracted from the coverphoto using OCR\n",
    "- Text extracted from the audio using speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2548cd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 1462 documents (23.99 MB)\n"
     ]
    }
   ],
   "source": [
    "TIKTOKS = JSON.parsefile(\"data/cleaned_master.json\",use_mmap=false)\n",
    "for tiktok in values(TIKTOKS)\n",
    "    tiktok[\"comment-text\"] = join([comment[\"comment-text\"] for comment in tiktok[\"comments\"]],\" \")\n",
    "end\n",
    "println(\"Corpus size: $(length(TIKTOKS)) documents ($(scifmt(Base.summarysize(TIKTOKS),digits=2,unit=\"B\")))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1956d3",
   "metadata": {},
   "source": [
    "# Normalization <a id=\"normalization\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658d585",
   "metadata": {},
   "source": [
    "One primary focus of our experiments will be different methods of tokenization: words vs character n-grams of varying length. We will construct a normalization function that can do either.\n",
    "\n",
    "Exploratory analysis reveals that TikTok descriptions and comments are rife with Unicode characters, including emojis, characters from foreign languages (Japanese is especially common among these TikToks as it is my second language), bold/italic/cursive letters, and other miscellaneous characters.\n",
    "\n",
    "The `Unidecode.jl` library converts emojis, Greek characters, and a small handful of other characters into English words describing the character; for example \"ðŸ˜µ\" is unidecoded to \":sweat_smile:\" and \"Î±\" is unidecoded to \"alpha.\"\n",
    "\n",
    "We will additionally use the `unidecode.py` Python library (via PyCall), which does not handle emojis, and decodes Greek characters to English letters instead of their names (e.g. \"Î±\" is decoded to \"a\" instead of \"alpha\"), but otherwise has a much broader range of applicability. Notably, it can decode Japanese characters to romaji (Latin characters), which will be important for  handling the few Japanese documents in the corpus, and it will ensure that the output is strictly ACSII.\n",
    "\n",
    "So, in order to ensure that we do not lose the emojis, we will use `Unidecode.jl` first, and then `unidecode.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23ea2a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normalize (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function normalize(document::String; \n",
    "        delim=r\"[\\s/â€”]+|-{2,}\",\n",
    "        replacements=[(r\"[\\s/â€”-]+\",\" \"), (r\"[^\\w@# ]\",\"\"), (!isascii,\"\")],\n",
    "        case_folding=lowercase,\n",
    "        filters = [token -> token==\"\"],\n",
    "        stopwords = Set(),\n",
    "        ngram_size = nothing\n",
    "    )\n",
    "    \"\"\"This function normalizes a text document, generating an array of\n",
    "    tokens by the following process:\n",
    "    \n",
    "    1. Decode Unicode characters and apply the specified `case_folding` function.\n",
    "    2. If n-grams size is not specified, use word tokenization:\n",
    "        2.1 Split on the specified delimiter (`delim`).\n",
    "        2.2 Apply the specified `replacements` to each token.\n",
    "    3. If n-gram size is specified, use n-gram tokenization:\n",
    "        3.1 Apply `replacements` to the entire document string.\n",
    "        3.2 Tokenize by generating each consecutive `ngram_size`-sized substring.\n",
    "    4. Apply the list of `filter` functions and remove invalid tokens and `stopwords`.\n",
    "    \"\"\"\n",
    "    document = unidecode(document)\n",
    "    #Use Unidecode.jl to decode emojis and Greek characters.\n",
    "    document = pyunidecode.unidecode(document)\n",
    "    #Use unidecode.py to decode the rest of the Unicode characters.\n",
    "    document = case_folding(document)\n",
    "    #Perform case folding.\n",
    "    \n",
    "    \n",
    "    tokens = String[]\n",
    "    if isnothing(ngram_size)\n",
    "        #If an n-gram size is not specified, then use word tokenization.\n",
    "        for token in split(document,delim)\n",
    "            #Split the text on the delimiters.\n",
    "            for (pattern, replacement) in replacements\n",
    "                token = replace(token, pattern => replacement)\n",
    "            end\n",
    "            #Perform all specified replacements.\n",
    "            push!(tokens,token)\n",
    "        end\n",
    "    else\n",
    "        #Otherwise, use n-gram tokenization.\n",
    "        for (pattern, replacement) in replacements\n",
    "            document = replace(document, pattern => replacement)\n",
    "        end\n",
    "        #With n-gram tokenization, replacements are done on the entire document.\n",
    "        if length(document) < ngram_size\n",
    "            #If the n-gram size is larger then the document, then just treat it as blank.\n",
    "        else \n",
    "            tokens = String[\n",
    "                join(document[i:j]) for (i,j) in\n",
    "                zip(1:(length(document)-ngram_size+1),ngram_size:length(document)+1)\n",
    "            ]\n",
    "        end\n",
    "    \n",
    "    end\n",
    "    \n",
    "    \n",
    "    valid_tokens = [token for token in tokens \n",
    "            if !(any(filter(token) for filter in filters)||(token in stopwords))]\n",
    "    #Only keep tokens that survive all the filters and are not stopwords.\n",
    "    \n",
    "    return valid_tokens\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240d5fd",
   "metadata": {},
   "source": [
    "Regardless of tokenization, I will fold to lower-case, and I won't remove any stopwords. \n",
    "\n",
    "With word tokenization, I will use the `[\\s/â€”-]+` pattern as the delimiter to split on any number of consecutive spaces, forward-slashe, em-dashes, or hyphens. With n-gram tokenization, I will replace matches to this pattern with a single space in have a single character to break up alphanumerics.\n",
    "\n",
    "All non-alphanumeric characters other than spaces, the @ symbol (used to denote usernames), and the # symbol (used in hashtags) will be removed. I will include a filter to catch empty tokens, as that can be an issue with the post-splitting replacement procedure used in word-tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae98e8d7",
   "metadata": {},
   "source": [
    "# Combinations of Features <a id=\"features\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845cfcf3",
   "metadata": {},
   "source": [
    "Another central focus of this research will be to identify which types of information are useful to include in the index. To this end, I will try enriching the base data with each of the 8 members of the power set of {title OCR text, speech recognition text, comment text} to see what conditions yield the best results. \n",
    "\n",
    "My hypothesis is that speech recognition and title OCR text will always be useful when they are available; comment text will be helfpul for content that is more nonverbal, but may be detrimental to overall results because they are often off-topic.\n",
    "\n",
    "Additionally, we will examine whether or not attributing terms to the fields from which they were derived will help. I think that the potential benefit of this is that it could prevent small n-grams in the basic information from being drowned out by occurrences of that same n-gram in a lower-quality field (i.e. comments, which are spammier than the other fields). It's worth noting that usernames and hashtags somewhat have their own built-in form of field attributions. Query terms will be tried against each field; the only benefit would be the increase in IDF in the document vector by splitting components by field of origin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "21e8adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "function attribute_field(tokens::Vector{String},fieldname::String)\n",
    "    \"\"\"Prefix each token with the fieldname and a colon.\"\"\"\n",
    "    return String[\"$(fieldname)$(token)\" for token in tokens]\n",
    "end\n",
    "\n",
    "\n",
    "function attribute_all_fields(query_tokens::Vector{String})\n",
    "    \"\"\"Prefix each query token with every attirbution label so that\n",
    "    postings from all fields can be used to compute similarity.\"\"\"\n",
    "    \n",
    "    return vcat(\n",
    "        query_tokens,#No prefix for basic info.\n",
    "        String[\"ocr:$(token)\" for token in query_tokens],\n",
    "        String[\"sr:$(token)\" for token in query_tokens],\n",
    "        String[\"c:$(token)\" for token in query_tokens]\n",
    "    )\n",
    "    \n",
    "end\n",
    "\n",
    "BASIC_FIELDS = [\"creator-username\",\"creator-nickname\",\"description\",\"music-title\"]\n",
    "ATTRIBUTION_PREFIXES = Dict(\n",
    "    \"speech-to-text\" => \"sr:\",\n",
    "    \"coverphoto-ocr\" => \"ocr:\",\n",
    "    \"comment-text\" => \"c:\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770b566",
   "metadata": {},
   "source": [
    "# Index Construction <a id=\"index-construction\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb7c35",
   "metadata": {},
   "source": [
    "Now, we have to implement the memory-based inversion algorithm. The function needs a few minor adjustments for this study because the docIDs are strings instead of integers, and we need to loop over multiple fields, possibly adding attribution prefixes. Otherwise, this is pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de9d6fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inverted_file_index (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function inverted_file_index(corpus::Dict; attributions=Dict(),fields=BASIC_FIELDS, norm_kwargs...)\n",
    "    \"\"\"This function constructs an inverted file index for an input corpus. The output \n",
    "    is a Dict of (term, postings list) pairs, where each postings list is a vector of \n",
    "    (docID, term frequency) pairs.\n",
    "    \n",
    "    The `fields` keyword specifies which fields of the corpus to index, and the `attributions`\n",
    "    keyword is used to assign respective field attribution prefixes, if any.\n",
    "    \"\"\"\n",
    "    index = Dict{String,Vector{Tuple{String,Int}}}()\n",
    "    #Initialize the index as an ordered dict that maps each term in the corpus \n",
    "    #to a postings list, which is a vector containing (docID, term frequency) pairs.\n",
    "    \n",
    "    for (id, document) in corpus\n",
    "        term_frequencies = Dict{String,Int}()\n",
    "        #Count each term as it occurs in the document.\n",
    "        for field in fields\n",
    "            for term in attribute_field(\n",
    "                    normalize(document[field]; norm_kwargs...),get(attributions,field,\"\"))\n",
    "                #Normalize the raw document; apply the specified field attribution prefix.\n",
    "                if haskey(term_frequencies,term)\n",
    "                    term_frequencies[term] += 1\n",
    "                else\n",
    "                    term_frequencies[term] = 1\n",
    "                end\n",
    "                #Tally each term as it appears in the normalized sequence of terms.\n",
    "            end\n",
    "        end\n",
    "\n",
    "        for (term,term_frequency) in term_frequencies\n",
    "            if haskey(index,term)\n",
    "                push!(index[term], (id,term_frequency))\n",
    "            else\n",
    "                index[term] = [(id,term_frequency)]\n",
    "            end\n",
    "            #If the term is already in the index, simply add a posting for this \n",
    "            #document. Otherwise, initialize the postings list with this posting.\n",
    "        end  \n",
    "        \n",
    "    end\n",
    "    return index\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e54c5ca",
   "metadata": {},
   "source": [
    "# Query Vectorization <a id=\"query-vectorization\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971588f1",
   "metadata": {},
   "source": [
    "I'll start by loading the queries from `queries.json`, which is formatted as a list of [querytext, target id] pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a8febf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test queries: 1\n"
     ]
    }
   ],
   "source": [
    "QUERIES = JSON.parsefile(\"queries.json\",use_mmap=false)\n",
    "println(\"Number of test queries: $(length(QUERIES))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd5c97",
   "metadata": {},
   "source": [
    "The last aspect we will focus on in our experiments is how cosine scoring is performed. Going forward, the notation I will use for different term weighting factors is consistent with that in [_Term-weighting approaches in automatic text retrieval_ (DOI: 10.1016/0306-4573(88)90021-0)](https://www.sciencedirect.com/science/article/abs/pii/0306457388900210).\n",
    "\n",
    "\n",
    "Most queries will likely be short and not meaningfully use any term more than once, so I think it is reasonable to assume that binary weighting on query terms will be fine. Normalization of the query vector doesn't matter because it will simply scale every score by the same factor, meaning the ranking will be unaffected. I will opt to normalize, so that the results will be between 0 and 1 when the document vector is also normalized, which will lend itself to interpretability.\n",
    "\n",
    "All of this is to say that we'll use the same formula (bxc) for weighting terms in query vectors. Our experimentation will only vary term weighting methods for document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0b7cf48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectorize_query (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function vectorize_query(query_tokens::Vector{String};\n",
    "        formula=(:b,:x,:c),n_docs=1,index=nothing,norm_kwargs...)\n",
    "    \"\"\"This function parses a query string as a term-space vector. The \n",
    "    `formula` keyword argument specifies which choice of term frequency, collection\n",
    "    frequency, and normalization factors to use when calculating the vector's components.\n",
    "    \n",
    "    If idf or probabilistic inverse collection frequency are used as the collection \n",
    "    frequency factor, then the number of documents in the target corpus and an inverted\n",
    "    index of the target corpus must be passed as keyword arguments.\n",
    "    \"\"\"\n",
    "    vector = Dict()\n",
    "    if formula[1] == :b\n",
    "        vector = Dict(term => 1.0 for term in unique(query_tokens))\n",
    "        #With binary weighting, each term that occurs gets a this-document factor of 1.\n",
    "    elseif formula[1] == :t\n",
    "        vector = countmap(query_tokens)\n",
    "        #With tf weighting, each term gets a this-document factor equal to the number\n",
    "        #of times it occurs.\n",
    "    elseif formula[1] == :n\n",
    "        count_map = countmap(query_tokens)\n",
    "        max_tf = maximum(values(count_map))\n",
    "        #Get the maximum tf for augmented normalized tf weighting.\n",
    "        vector = Dict(term => 0.5*(1+tf/max_tf) for (term,tf) in count_map)\n",
    "        #Use the formula for augmented normalized tf weighting.\n",
    "    end\n",
    "    \n",
    "    if formula[2] == :x\n",
    "        #If the collection factor is just 1, we don't need to do anything.\n",
    "    elseif formula[2] == :f\n",
    "        vector = Dict(\n",
    "            term => component * log2(n_docs/length(get(index,term,[])))\n",
    "            for (term, component) in vector\n",
    "        )\n",
    "        #The collection-frequency factor is equal to idf, which is the base-2\n",
    "        #log of the total number of documents divided by the number of documents\n",
    "        #that term occurs in.\n",
    "    elseif formula[2] == :p\n",
    "        vector = Dict(\n",
    "            term => component * log2(\n",
    "                (n_docs-length(get(index,term,[])))/length(get(index,term,[]))\n",
    "            )\n",
    "            for (term, component) in vector\n",
    "        )\n",
    "        #Probabilistic inverse collection frequency just involves subtracting the\n",
    "        #df from the total number of documents (the numerator of the idf expression)\n",
    "    end\n",
    "    \n",
    "    if formula[3] == :x\n",
    "        #No normalization\n",
    "    elseif formula[3] == :c\n",
    "        normalization_factor = sqrt( sum(component^2 for component in values(vector)) )\n",
    "        vector = Dict(key => value/normalization_factor for (key, value) in vector)\n",
    "        #Divide each component by the magnitude of the vector.\n",
    "    end\n",
    "    \n",
    "    return vector\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c901f",
   "metadata": {},
   "source": [
    "# Cosine Scoring <a id=\"consine-scoring\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8515a8",
   "metadata": {},
   "source": [
    "In order to perform cosine scoring with normalization, we need to compute lengths of document vectors. Instead of doing this once per query, we only need to do it once per experiment (every time the index and/or document vector weighting formula change). So, I've implemented a function that will perform this calculation so we can store the results.\n",
    "\n",
    "Note: because augmented normalized term frequency weighting (the :n term weight) requires computing max term frequency for every document, which is a bit convoluted, I've opted to exclude it from our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f86e2081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_vector_lengths (generic function with 1 method)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function document_vector_lengths(index::Dict, corpus_size::Int;formula=(:t,:f,:x))\n",
    "    \"\"\"This function computes for each document in a corpus the magnitude of the term-space \n",
    "    vector of that document.\n",
    "    \n",
    "    The `formula` keyword argument specifies which choice of term frequency, collection\n",
    "    frequency, and normalization factors to use when calculating the vector's components.\"\"\"\n",
    "    vector_lengths = Dict{String,Float64}()\n",
    "    \n",
    "    for (term,postings) in index\n",
    "        for (docid,tf) in postings\n",
    "            component = 0\n",
    "            if formula[1] == :b\n",
    "                component = 1\n",
    "            elseif formula[1] == :t\n",
    "                component = tf\n",
    "            end\n",
    "            #Term weighting factor.\n",
    "            \n",
    "            if formula[2] == :x\n",
    "            elseif formula[2] == :f\n",
    "                component *= log2(corpus_size)/length(postings)\n",
    "            elseif formula[2] == :p\n",
    "                component *= log2(\n",
    "                    (corpus_size-length(postings))/length(postings)\n",
    "                )\n",
    "            end \n",
    "            #Document weighting factor.\n",
    "            \n",
    "            if docid âˆˆ values(vector_lengths)\n",
    "                vector_lengths[docid] += component^2\n",
    "            else \n",
    "                vector_lengths[docid] = component^2\n",
    "            end\n",
    "            #For each docid, we must first compute the sum of squared components. \n",
    "        end\n",
    "    end\n",
    "    return Dict{String,Float64}(docid => sqrt(sum_sq) for (docid,sum_sq) in vector_lengths)\n",
    "    #Take the square root of the sum of squared term-frequency components.\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025508a5",
   "metadata": {},
   "source": [
    "The following function performs cosine scoring and returns a ranked list of docids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb79be21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cosine_score (generic function with 1 method)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cosine_score(\n",
    "        query_tokens::Vector{String}, index::Dict, corpus_size::Int;\n",
    "        query_formula = (:b,:x,:c), document_formula = (:t,:f,:c),\n",
    "        document_veclengths=nothing\n",
    "    )\n",
    "    \"\"\"This function computes vector similarity scores between a query and a corpus\n",
    "    and returns list of docids, sorted in rank order.\n",
    "    \n",
    "    The `query_formula` and `document_formula` keyword arguments specify how term-space\n",
    "    vector components for the query and the documents in the corpus are computed, using\n",
    "    the notation provided by Salton and Buckley.\n",
    "    \n",
    "    Vector-magnitude normalization requires vector lengths for each corpus document. These \n",
    "    can be passed as keyword arguments to save time, or can be computed on-the-fly otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    query_vector = vectorize_query(query_tokens,formula=query_formula,index=index,n_docs=corpus_size)\n",
    "    #Parse the query into a term-space vector.\n",
    "    \n",
    "    sim_scores = Dict{String,Float64}()\n",
    "    \n",
    "    for (term, q_component) in query_vector\n",
    "        #Loop over each term in the query vector.\n",
    "        for (docid,tf) in get(index,term,[])\n",
    "            #Compute the term-space vector component corresponding\n",
    "            #to the given term for each document in that term's postings list.\n",
    "            if (document_formula[2]==:f)&&(document_veclengths[docid]==0)\n",
    "                continue\n",
    "                #Skip documents with length 0.\n",
    "            end\n",
    "            d_component = 0\n",
    "            if document_formula[1] == :b\n",
    "                d_component = 1\n",
    "                #If the docid is in the postings list, then the term must be \n",
    "                #present in the document.\n",
    "            elseif document_formula[1] == :t\n",
    "                d_component = tf\n",
    "                #Tf weighting.\n",
    "            end\n",
    "            #Compute the term frequency factor.\n",
    "            \n",
    "            if document_formula[2] == :x\n",
    "                #No collection frequency weighting.\n",
    "            elseif document_formula[2] == :f\n",
    "                d_component *= log2(length(document_veclengths)/length(get(index,term,[])))\n",
    "                #Multiply by the idf.\n",
    "            elseif document_formula[2] == :p\n",
    "                d_component *= log2(\n",
    "                    (length(document_veclengths)-length(get(index,term,[])))/length(get(index,term,[]))\n",
    "                )\n",
    "                #Multiply by the idf. \n",
    "            end \n",
    "                \n",
    "            if document_formula[3] == :x\n",
    "                #No normalization.\n",
    "            elseif document_formula[3] == :c\n",
    "                d_component /= document_veclengths[docid]\n",
    "                #Divide by the length of the document's term-space vector.\n",
    "            end\n",
    "            \n",
    "            if docid in keys(sim_scores)\n",
    "                sim_scores[docid] += q_component*d_component\n",
    "            else\n",
    "                sim_scores[docid] = q_component*d_component\n",
    "            end\n",
    "            #Multiply the corresponding component of the query and document's \n",
    "            #term-space vectors together and add to the running total (similarity score).\n",
    "            \n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return sort!(collect(keys(sim_scores)), by=(docid->sim_scores[docid]), rev=true)\n",
    "    #Sort the similarity scores in decreasing order.\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afcbc9",
   "metadata": {},
   "source": [
    "# Experiments <a id=\"experiments\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c03c96",
   "metadata": {},
   "source": [
    "Now all that's left is to run the experiments. To quickly recap what's been discussed so far, we comparing the following methodologies:\n",
    "\n",
    "- n-gram vs word tokenization (including n-gram size)\n",
    "    - Because documents are relatively short, they're less likely to have multiple morphological variants of a given word. Additionally, because these data are obtained from social media, they are more likely to have spelling errors. These factors both suggest that n-gram tokenization may be more helpful.\n",
    "- Combinations of different features: basic + 2^{speech to text, coverphoto ocr, comment text}\n",
    "    - Comments may help provide context to videos we little to no recognizable speech and/or sparse description. However, it's possible that off-topic comments may cause irrelevant documents to rank higher, reducing precision. We expect Coverphoto OCR and speech recognition to always be helpful (or never harmful at the least).\n",
    "- Field attribution (i.e. labeling terms based on which feature they were derived from)\n",
    "    - We have set it up such that queries will match with a term regardless of which feature it came from, so field attributions will only have an impact on a document term's IDF. We expect that this should help mitigate the negative impact of off-topic comment text, which tends to be more verbose than the basic information.\n",
    "\n",
    "\n",
    "Additionally, we will examine the following methods for computing document vector components:\n",
    "- Raw (:t) vs binary (:b) term weighting\n",
    "    - Possible that :b is more useful because :t will be biased toward longer videos\n",
    "- None (:x) vs IDF (:f) vs probabilistic IDF (:p)\n",
    "    - Most likely that :f or :p is best because they give higher weight to rarer terms\n",
    "- None (:x) vs Euclidean (:c) normalization\n",
    "    - Possible that :x is better than :c if documents are diluted by long, useless comments\n",
    "\n",
    "\n",
    "There are two common use-cases for TikTok retrieval: searching for an individual TikTok that a user vaguely remembers but lost track of, and searching for TikToks related to a trend or theme. The platform's search engine usually works pretty well in the latter case; our goal is to improve the former. In other words, we are trying to optimize the ability to recall specific videos.\n",
    "\n",
    "The test queries were constructed to this end, and they thus only have a single relevant retrieval target. Because we are only dealing with a single relevant document for each query, the metric we will use to evaluate performance is the average rank of the target in the similarity-ranked list. In the best case, this metric will be close to 1. If the retrieval system were to rank documents at random, we would expect the metric to be around ~731 (the midpoint of the list). If the metric is close to 1462, then we are probably sorting the ranked list backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "37bafaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 indices to be generated\n",
      "12 combinations of scoring formulae to be used on each\n",
      "960 total experiments to be performed\n"
     ]
    }
   ],
   "source": [
    "ngram_sizes = [nothing,4,5,6,7]\n",
    "features = collect(powerset([\"speech-to-text\",\"comment-text\",\"coverphoto-ocr\"]))\n",
    "index_iterator = Iterators.product(ngram_sizes,[true,false],features)\n",
    "println(\"$(length(index_iterator)) indices to be generated\")\n",
    "\n",
    "formula_iterator = Iterators.product([:b,:t],[:x,:f,:p],[:x,:c])\n",
    "println(\"$(length(formula_iterator)) combinations of scoring formulae to be used on each\")\n",
    "\n",
    "\n",
    "println(\"$(length(formula_iterator)*length(index_iterator)) total experiments to be performed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8bba5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_EXPERIMENTS = true;\n",
    "#Toggles whether to run the experiments or skip to the analysis \n",
    "#(in case you refresh the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ec212999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimenting on index 1 / 80\n",
      "Experimenting on index 2 / 80\n",
      "Experimenting on index 3 / 80\n",
      "Experimenting on index 4 / 80\n",
      "Experimenting on index 5 / 80\n",
      "Experimenting on index 6 / 80\n",
      "Experimenting on index 7 / 80\n",
      "Experimenting on index 8 / 80\n",
      "Experimenting on index 9 / 80\n",
      "Experimenting on index 10 / 80\n",
      "Experimenting on index 11 / 80\n",
      "Experimenting on index 12 / 80\n",
      "Experimenting on index 13 / 80\n",
      "Experimenting on index 14 / 80\n",
      "Experimenting on index 15 / 80\n",
      "Experimenting on index 16 / 80\n",
      "Experimenting on index 17 / 80\n",
      "Experimenting on index 18 / 80\n",
      "Experimenting on index 19 / 80\n",
      "Experimenting on index 20 / 80\n",
      "Experimenting on index 21 / 80\n",
      "Experimenting on index 22 / 80\n",
      "Experimenting on index 23 / 80\n",
      "Experimenting on index 24 / 80\n",
      "Experimenting on index 25 / 80\n",
      "Experimenting on index 26 / 80\n",
      "Experimenting on index 27 / 80\n",
      "Experimenting on index 28 / 80\n",
      "Experimenting on index 29 / 80\n",
      "Experimenting on index 30 / 80\n",
      "Experimenting on index 31 / 80\n",
      "Experimenting on index 32 / 80\n",
      "Experimenting on index 33 / 80\n",
      "Experimenting on index 34 / 80\n",
      "Experimenting on index 35 / 80\n",
      "Experimenting on index 36 / 80\n",
      "Experimenting on index 37 / 80\n",
      "Experimenting on index 38 / 80\n",
      "Experimenting on index 39 / 80\n",
      "Experimenting on index 40 / 80\n",
      "Experimenting on index 41 / 80\n",
      "Experimenting on index 42 / 80\n",
      "Experimenting on index 43 / 80\n",
      "Experimenting on index 44 / 80\n",
      "Experimenting on index 45 / 80\n",
      "Experimenting on index 46 / 80\n",
      "Experimenting on index 47 / 80\n",
      "Experimenting on index 48 / 80\n",
      "Experimenting on index 49 / 80\n",
      "Experimenting on index 50 / 80\n",
      "Experimenting on index 51 / 80\n",
      "Experimenting on index 52 / 80\n",
      "Experimenting on index 53 / 80\n",
      "Experimenting on index 54 / 80\n",
      "Experimenting on index 55 / 80\n",
      "Experimenting on index 56 / 80\n",
      "Experimenting on index 57 / 80\n",
      "Experimenting on index 58 / 80\n",
      "Experimenting on index 59 / 80\n",
      "Experimenting on index 60 / 80\n",
      "Experimenting on index 61 / 80\n",
      "Experimenting on index 62 / 80\n",
      "Experimenting on index 63 / 80\n",
      "Experimenting on index 64 / 80\n",
      "Experimenting on index 65 / 80\n",
      "Experimenting on index 66 / 80\n",
      "Experimenting on index 67 / 80\n",
      "Experimenting on index 68 / 80\n",
      "Experimenting on index 69 / 80\n",
      "Experimenting on index 70 / 80\n",
      "Experimenting on index 71 / 80\n",
      "Experimenting on index 72 / 80\n",
      "Experimenting on index 73 / 80\n",
      "Experimenting on index 74 / 80\n",
      "Experimenting on index 75 / 80\n",
      "Experimenting on index 76 / 80\n",
      "Experimenting on index 77 / 80\n",
      "Experimenting on index 78 / 80\n",
      "Experimenting on index 79 / 80\n",
      "Experimenting on index 80 / 80\n",
      "3746.098806 seconds (2.24 G allocations: 90.923 GiB, 1.12% gc time, 0.02% compilation time: 13% of which was recompilation)\n"
     ]
    }
   ],
   "source": [
    "if RUN_EXPERIMENTS\n",
    "@time begin    \n",
    "n_indices = length(index_iterator)\n",
    "n_experiments = n_indices * length(formula_iterator)\n",
    "experiment_id = 0\n",
    "CORPUS_SIZE = length(TIKTOKS)\n",
    "\n",
    "db = SQLite.DB(\"results.db\")\n",
    "DBInterface.execute(db,\"DELETE FROM indexes;\")\n",
    "DBInterface.execute(db,\"DELETE FROM experiments;\")\n",
    "DBInterface.execute(db,\"DELETE FROM ranking_results;\")\n",
    "#Erase old results.\n",
    "\n",
    "for (index_id,(ngram_size,attributions,features)) in enumerate(index_iterator)\n",
    "    \n",
    "    println(\"Experimenting on index $(index_id) / $(n_indices)\")\n",
    "    \n",
    "    index = nothing\n",
    "    \n",
    "    indexing_time = @elapsed begin\n",
    "        index = inverted_file_index(\n",
    "            TIKTOKS; \n",
    "            attributions = attributions ? ATTRIBUTION_PREFIXES : Dict(),\n",
    "            fields = vcat(BASIC_FIELDS,features), \n",
    "            ngram_size = ngram_size\n",
    "        )\n",
    "    end\n",
    "    \n",
    "    \n",
    "    DBInterface.execute(\n",
    "        db,\n",
    "        \"\"\"INSERT INTO indexes(index_id,ngram_size,field_attributions,sr,ocr,\n",
    "        comments,indexing_time,storage)\n",
    "        VALUES (?,?,?,?,?,?,?,?);\"\"\",\n",
    "        [index_id,ngram_size,attributions,(\"speech-to-text\"âˆˆfeatures),(\"coverphoto-ocr\"âˆˆfeatures),\n",
    "        (\"comment-text\" âˆˆ features), indexing_time, Base.summarysize(index)]\n",
    "    )\n",
    "    \n",
    "    normalized_queries = [\n",
    "        (attributions ? attribute_all_fields(normalize(query[\"text\"];ngram_size=ngram_size)) : \n",
    "            normalize(query[\"text\"];ngram_size=ngram_size), query[\"target\"])\n",
    "        for query in QUERIES\n",
    "    ]\n",
    "    #If the index uses field attributions, ensure that the query search for matches in all fields.\n",
    "    \n",
    "    \n",
    "    for (term_weight,collection_weight,normalizer) in formula_iterator\n",
    "        experiment_id += 1\n",
    "         DBInterface.execute(\n",
    "            db,\n",
    "            \"\"\"INSERT INTO experiments(experiment_id,index_id,term_weight,\n",
    "            collection_weight,normalizer) VALUES (?,?,?,?,?);\"\"\",\n",
    "            [experiment_id, index_id, string(term_weight),\n",
    "                string(collection_weight), string(normalizer)]\n",
    "        )\n",
    "        \n",
    "        document_veclengths = document_vector_lengths(index, CORPUS_SIZE;\n",
    "            formula=(term_weight,collection_weight,normalizer), max_tfs=nothing)\n",
    "        \n",
    "        for (query_id,(query_tokens, target_id)) in enumerate(normalized_queries)\n",
    "            \n",
    "            target_rank = Inf\n",
    "            retrieval_time = @elapsed begin\n",
    "                \n",
    "                ranked_list = cosine_score(query_tokens, index, CORPUS_SIZE;\n",
    "                    document_formula = (term_weight,collection_weight,normalizer),\n",
    "                    document_veclengths=document_veclengths)\n",
    "                    target_rank = findfirst(ranked_list.==target_id)\n",
    "            \n",
    "            end\n",
    "            \n",
    "            DBInterface.execute(\n",
    "            db,\n",
    "            \"\"\"INSERT INTO ranking_results(experiment_id,query_id,retrieval_time,target_rank) \n",
    "                VALUES (?,?,?,?);\"\"\",\n",
    "            [experiment_id, query_id, retrieval_time, target_rank]\n",
    "        )\n",
    "        end\n",
    "        \n",
    "    end\n",
    "       \n",
    "end\n",
    "\n",
    "\n",
    "DBInterface.close!(db)\n",
    "end    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e1463",
   "metadata": {},
   "source": [
    "# Analyses <a id=\"analyses\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6bce9",
   "metadata": {},
   "source": [
    "In this section, we'll take a look into the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0d031",
   "metadata": {},
   "source": [
    "## Index Construction & Query Time <a id=\"anal-time\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c544aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLite.DB(\"results.db\")\n",
    "indexes = DBInterface.execute(db,\"SELECT * FROM indexes;\") |> DataFrame\n",
    "DBInterface.close!(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851cedc",
   "metadata": {},
   "source": [
    "We'll start out by looking at a simple histogram of all the index construction times to get a sense of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8756ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(\n",
    "    indexes[:,\"indexing_time\"],\n",
    "    title=\"Distribution of Index Construction Time\",\n",
    "    xlabel=\"Time (s)\",\n",
    "    ylabel=\"Count\",\n",
    "    legend=false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d2e50",
   "metadata": {},
   "source": [
    "Now, let's examine the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2372764",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(\n",
    "    player_stats[winrates_order,\"win_rate\"],\n",
    "    xticks = (1:nrow(player_stats), player_stats[winrates_order,\"username\"]),\n",
    "    ylims = (0,1),\n",
    "    title = \"Win Rates of Training Set Contributors\",\n",
    "    xlabel = \"Username\",\n",
    "    ylabel = \"Win Rate\",\n",
    "    legend = false\n",
    ")\n",
    "\n",
    "hline!([0.5],color=:black,label=\"0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a525a9",
   "metadata": {},
   "source": [
    "## Index Storage <a id=\"anal-storage\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8021bd3",
   "metadata": {},
   "source": [
    "## Performance vs Query Properties <a id=\"anal-queries\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55e1de",
   "metadata": {},
   "source": [
    "## Performance vs Indexing Methods <a id=\"anal-indexing\"></a>\n",
    "### [^Contents](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7ede43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.067492 seconds (1.46 M allocations: 69.372 MiB, 99.49% compilation time)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@time twoktests_corpus = load_duplicatetests_corpus(\"twoktests/twok.tsv\")\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16cd0511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.696330 seconds (2.15 M allocations: 602.634 MiB, 4.18% gc time, 5.18% compilation time)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CORPUS_NAMES = [\n",
    "    \"thirty\",\"hundred\",\"threehundred\",\"onek\",\n",
    "    \"threek\",\"tenk\",\"thirtyk\",\"hundredk\"\n",
    "]\n",
    "@time duplicatetests_corpora = Dict(\n",
    "    name => load_duplicatetests_corpus(\"duplicatetests/$(name).tsv\")\n",
    "    for name in CORPUS_NAMES\n",
    ")\n",
    "println()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c98455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>9Ã—3 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Corpus Name</th><th style = \"text-align: left;\"># Documents</th><th style = \"text-align: left;\">Usage</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"String\" style = \"text-align: left;\">String</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">thirty</td><td style = \"text-align: right;\">30</td><td style = \"text-align: left;\">129.06 kB</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">hundred</td><td style = \"text-align: right;\">100</td><td style = \"text-align: left;\">427.69 kB</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">threehundred</td><td style = \"text-align: right;\">300</td><td style = \"text-align: left;\">1.2 MB</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">onek</td><td style = \"text-align: right;\">1000</td><td style = \"text-align: left;\">4.0 MB</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">threek</td><td style = \"text-align: right;\">3000</td><td style = \"text-align: left;\">11.9 MB</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">tenk</td><td style = \"text-align: right;\">10000</td><td style = \"text-align: left;\">39.27 MB</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">thirtyk</td><td style = \"text-align: right;\">30000</td><td style = \"text-align: left;\">117.33 MB</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">hundredk</td><td style = \"text-align: right;\">100000</td><td style = \"text-align: left;\">390.86 MB</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">twok</td><td style = \"text-align: right;\">2000</td><td style = \"text-align: left;\">8.11 MB</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& Corpus Name & \\# Documents & Usage\\\\\n",
       "\t\\hline\n",
       "\t& String & Int64 & String\\\\\n",
       "\t\\hline\n",
       "\t1 & thirty & 30 & 129.06 kB \\\\\n",
       "\t2 & hundred & 100 & 427.69 kB \\\\\n",
       "\t3 & threehundred & 300 & 1.2 MB \\\\\n",
       "\t4 & onek & 1000 & 4.0 MB \\\\\n",
       "\t5 & threek & 3000 & 11.9 MB \\\\\n",
       "\t6 & tenk & 10000 & 39.27 MB \\\\\n",
       "\t7 & thirtyk & 30000 & 117.33 MB \\\\\n",
       "\t8 & hundredk & 100000 & 390.86 MB \\\\\n",
       "\t9 & twok & 2000 & 8.11 MB \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m9Ã—3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0mâ”‚\u001b[1m Corpus Name  \u001b[0m\u001b[1m # Documents \u001b[0m\u001b[1m Usage     \u001b[0m\n",
       "     â”‚\u001b[90m String       \u001b[0m\u001b[90m Int64       \u001b[0m\u001b[90m String    \u001b[0m\n",
       "â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "   1 â”‚ thirty                 30  129.06 kB\n",
       "   2 â”‚ hundred               100  427.69 kB\n",
       "   3 â”‚ threehundred          300  1.2 MB\n",
       "   4 â”‚ onek                 1000  4.0 MB\n",
       "   5 â”‚ threek               3000  11.9 MB\n",
       "   6 â”‚ tenk                10000  39.27 MB\n",
       "   7 â”‚ thirtyk             30000  117.33 MB\n",
       "   8 â”‚ hundredk           100000  390.86 MB\n",
       "   9 â”‚ twok                 2000  8.11 MB"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(OrderedDict(\n",
    "    \"Corpus Name\" => [CORPUS_NAMES...,\"twok\"],\n",
    "    \"# Documents\" => [\n",
    "        length(corpus) \n",
    "        for corpus in vcat(\n",
    "            [duplicatetests_corpora[n] for n in CORPUS_NAMES],\n",
    "            twoktests_corpus\n",
    "        ) \n",
    "    ],\n",
    "    \"Usage\" => [\n",
    "        scifmt(Base.summarysize(corpus),digits=2,unit=\"B\") \n",
    "        for corpus in vcat(\n",
    "            [duplicatetests_corpora[n] for n in CORPUS_NAMES],\n",
    "            twoktests_corpus\n",
    "        ) \n",
    "    ]\n",
    "    \n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364a970",
   "metadata": {},
   "source": [
    "Looks about right. Let's move on."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "13c14d6b64064c7c8121b4671a505c45",
   "lastKernelId": "a37cc26f-0fe6-441e-9a89-bc2e2fe374d9"
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
